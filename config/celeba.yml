training:
  batch_size: 8
  n_epochs: 20
  n_iters: 2000001
  ngpu: 1
  snapshot_freq: 10000

data:
  ## mnist
#  dataset: "MNIST"
#  image_size: 28
#  channels: 1
#  logit_transform: false
#  random_flip: false
  ##  celeba
  dataset: "CELEBA"
  image_size: 128
  channels: 3
  out_channels: 2 
  logit_transform: false
  random_flip: true

 # # nyuv2
 #  dataset: "NYUv2"
 #  image_size: 64
 #  channels: 4
 #  out_channels: 1
 #  logit_transform: false
 #  random_flip: false

model:
  sigma_max: 0.8
  sigma_min: 0.1
  pre_train: True
  ## configurations for CelebA, CIFAR10
  num_scales: 20
  ngf: 128


optim:
  weight_decay: 0.001
  optimizer: "Adam"
  lr: 0.0001
  beta1: 0.9
  amsgrad: false
